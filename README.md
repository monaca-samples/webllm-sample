# WebLLM ブラウザテスト

ブラウザ上でLLM（大規模言語モデル）を実行するシンプルなデモプロジェクトです。

## 概要

- ブラウザだけでLLMを実行できます（サーバー不要）
- WebGPUを活用してクライアント側で推論を実行
- 日本語入力に対応（IME変換の問題を解決済み）

## 使い方

1. ローカルサーバーを起動（例: `python -m http.server`）
2. WebGPU対応ブラウザで `http://localhost:8000` にアクセス
3. 「モデルをロード」をクリックして待機
4. メッセージを入力して「送信」または「ストリーミング送信」をクリック

## 必要環境

- Chrome/Edge 113以降、またはWebGPU対応ブラウザ
- インターネット接続（初回モデルダウンロード時）
- 十分なストレージ容量（モデルキャッシュ用）

## 注意点

- 初回はモデルのダウンロードに時間がかかります
- 使用モデル: Qwen2.5-1.5B-Instruct（軽量モデル）
- 小さなモデルのため、回答品質には限界があります

## 参考

- [WebLLM公式サイト](https://webllm.mlc.ai/)
- [MLC Models](https://mlc.ai/models)